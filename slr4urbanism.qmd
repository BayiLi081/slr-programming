---
title: "Systematic Literature Review for Urbanism"
author: "Bayi Li"
date: `r Sys.Date()`
output: html
---

## Setup

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(pdftools)
library(revtools)
```

## Extract data

<!-- Add text extraction script or reference to Python code -->

## Load data (already extracted from PDFs)


## 1. Mapping case study locations

Input: 
- manual
- extracting location from title
- extracting location from abstract
- if multiple locations are mentioned, multiple records are created

Output:
- interactive map with Leaflet
- tiled grid map with template available for QGIS as well

Required packages: leaflet, rgdal, sp, rgeos, maptools, ggplot2, ggthemes, ggmap, gridExtra, grid, RColorBrewer, scales, viridis, viridisLite, mapview, tmap, tmaptools, tmapthemes, tmaputi

```{r libraries, echo=FALSE, message=FALSE, warning=FALSE}
library(geojsonio)
library(leaflet)
# plot the selected are with sp
library(sp)
```

```{r}
spdf <- geojson_read("data/map_basis/Tile-Grid-Map-Cleaned.geojson",  what = "sp")
```



```{r map, echo=FALSE, message=FALSE, warning=FALSE}
par(mar=c(0,0,0,0))
plot(spdf, col="grey")
# add labels using the column 'alpha.2' from the spdf data frame, the x y columns from 'coord_x' and 'coord_y'
text(spdf$coord_x, spdf$coord_y, labels = spdf$alpha.2, cex = 0.5)
```




```{r map, echo=FALSE, message=FALSE, warning=FALSE}




## 2. Keyword wordclouds

Input: keywords

Output: Word clouds - can be overall or per time period

Required packages: wordcloud, (wordcloud2), RColorBrewer, tm

```{r libraries, echo=FALSE, message=FALSE, warning=FALSE}
library(wordcloud)
library(RColorBrewer)
library(wordcloud2)
library(tm)
```

Read the data. 

To explore the keywords, we need to extract the three columns "id", "Keywords.Plus", "Year" from the data frame.

```{r data, echo=FALSE, message=FALSE, warning=FALSE}
df <- read.csv("data/collection/final_combined.csv", header = TRUE, sep = ",")

# extract the three columns "id", "Keywords.Plus", "Year"
df_1 <- df[, c("id", "Keywords.Plus", "Year")]
```

```{r data cleaning, echo=FALSE, message=FALSE, warning=FALSE}
# rename "Keywords.Plus" to "Keywords"
names(df_1)[names(df_1) == "Keywords.Plus"] <- "Keywords"

# remove the rows with NA or empty in the column "Keywords"
df_1 <- df_1[!is.na(df_1$Keywords) & df_1$Keywords != "", ]

# reset the row index
rownames(df_1) <- NULL

df_1
```

```{r corpus, echo=FALSE, message=FALSE, warning=FALSE}
# create a corpus
corpus <- Corpus(VectorSource(df_1$Keywords))

# convert the corpus to lower case
corpus <- tm_map(corpus, tolower)

# remove punctuation
corpus <- tm_map(corpus, removePunctuation)

# remove numbers
corpus <- tm_map(corpus, removeNumbers)

# remove whitespace
corpus <- tm_map(corpus, stripWhitespace)

# remove stopwords (i.e., less than 3 characters)
corpus <- tm_map(corpus, removeWords, c("segregation", "and", "the", "for", "are", "was", "has", "can", "not", "but", "all", "any", "this", "that", "with", "from", "which", "who", "how", "why", "when", "where", "what", "who", "will", "would", "could", "should", "may", "might", "must", "shall", "should", "have", "had", "has", "been", "were", "was", "be", "is", "are", "a", "an", "of", "on", "in", "to", "as", "at", "by", "or", "if", "it", "its", "their", "they", "them", "there", "then", "than", "so", "no", "yes", "up", "down", "out", "into", "over", "under", "between", "through", "about", "after", "before", "between", "under", "over", "above", "below", "through", "across", "against", "along", "around", "behind", "beneath", "beside", "besides", "beyond", "during", "except", "inside", "near", "off", "onto", "outside", "since", "toward", "underneath", "until", "upon", "within", "without", "upon"))
```

```{r dtm, echo=FALSE, message=FALSE, warning=FALSE}
# create a document term matrix
dtm <- TermDocumentMatrix(corpus)

# convert the document term matrix to a matrix
m <- as.matrix(dtm)

# calculate the frequency of each word
words <- sort(rowSums(m), decreasing = TRUE)

# create a data frame with two columns: word and frequency
df1_feq <- data.frame(word = names(words), freq = words)

# delete intermedia variables
rm(corpus, dtm, m, words)
```


```{r wordcloud, echo=FALSE, message=FALSE, warning=FALSE}
set.seed(1234) # for reproducibility 
wordcloud(words = df1_feq$word, freq = df1_feq$freq, min.freq = 2, max.words=200, random.order=FALSE, rot.per=0.35,            colors=brewer.pal(8, "Dark2"))
```




## 3. Text mining

Input: Full text, machine readable (this requires text to be extracted from PDFs prior to analysis)

### 3.1 Word cooccureence

What questions co-occur with a specific word of interest, e.g., segregation?

- using the LDA model and top words
- using word embeddings

Output: A bar chart with top words related to the word of interest

### 3.2 Topic modeling

- using LDA to identify dominant topics in a corpus of papers

#### 3.2.1 Topics across the corpus

Output: Bar charts showing top words in each topic

#### 3.2.2 Evolution of topics

Output: A line chart showing the relative importance of topics for a given period.












```{r extracttxt}
files <- list.files(path = "data/collection/PDFs", pattern = "*.pdf", full.names = TRUE)

my_list_output <- list()

for (file in files){
  # Extract text from the PDF
  text <- pdftools::pdf_text(file)
  # Split each line of text for every new line
  lines <- strsplit(text, "\n")[[1]]
  # list to store each new line of information in a key value pair
  info_list <- lapply(strsplit(lines, ": "), function(x) setNames(x[1], x[2]))
  # take the output lines in a single column of a data frame
  df <- do.call(rbind, info_list)
  # remove rows with NA
  df_cleaned <- na.omit(df)
  # select 4th row to end of the data frame
  df_rows <- data.frame(df_cleaned[4:nrow(df_cleaned), ])
  # rename single column of the dataframe
  names(df_rows) = c("Values")
  # remove leading and trailing spaces
  df_rows$Values <- trimws(df_rows$Values)
  # remove leading and trailing spaces
  df_rows$Values <- str_replace_all(df_rows$Values, "\\s+", " ")
  
  # select each row which is currently a character string separated by spaces
  # split each character from the string and recombine them with "-" separator
  # in the process we need to unlist and list the observations of each row
  process_row <- function(row) {
    mylist = row["Values"]
    mylist1 = paste0(unlist(as.list(unlist(strsplit(mylist, '[[:space:]]')))), collapse = "-")
    return(mylist1)
  }
  
  # apply the above function for all rows in the data-frame
  df_rows$ConcatenatedValues <- apply(df_rows, 1, process_row)
  
  # take 1st row value of df_cleaned which looks like unique identifier of the file
  id_value = df_cleaned[[1]]
  
  # extract the metric enclosed within parenthesis ()
  # find position of the string )- after which the metric value is mentioned for each row
  # extract the string of metric value bounded by "-" before and after
  # replace "," with decimal dot for each metric value
  # convert character string to numeric
  # transpose the metric as key and corresponding values grouped by the id variable
  # remove the second column which looks to be NA
  df_transformed = df_rows %>%
    extract(ConcatenatedValues, into = c("Metric"), regex = "\\(([^)]+)\\)", remove = FALSE) %>%
    mutate(StringPosition = str_locate(ConcatenatedValues, "\\)-")) %>%
    mutate(len = nchar(ConcatenatedValues)) %>%
    mutate(string = str_sub(ConcatenatedValues, start=StringPosition[,"end"], end=len)) %>%
    extract(string, into = c("Metric_Value"), regex = "\\-([^)]+)\\-", remove = FALSE) %>%
    select(Values, Metric, Metric_Value) %>%
    mutate(Metric_Value = str_replace(Metric_Value, ",", ".")) %>%
    mutate(Metric_Value = as.numeric(Metric_Value)) %>%
    mutate(id = id_value) %>%
    select(id, Metric, Metric_Value) %>%
    pivot_wider(names_from = Metric, values_from = Metric_Value) %>%
    select(-2)
  
  # store the output of each file in an empty list which can be accessed based on index
  my_list_output[[file]] <- df_transformed
}

## Remove unwanted objects from environment
rm(file, files, id_value, lines, text)
rm(df_rows, df_cleaned, df, info_list, process_row)

my_list_output <- apply(my_list_output,2,as.character)

# save the list as txt
write.table(my_list_output, file = "data/collection/textdf.txt", sep = "\t", row.names = FALSE)
```

```{r extracttxt}
fs::dir_ls("data/collection/PDFs") %>% 
  as.character() %>% 
  as_tibble() %>% 
  mutate(text = map_chr(value, function(x){
    pdftools::pdf_text(x) %>% 
      paste0(collapse = "") %>% 
      str_remove_all("[\\s\\n\\t\\d[a-z].]")
  })) -> textdf

textdf %>% 
  write_csv("data/collection/textdf.csv")

textdf
```



